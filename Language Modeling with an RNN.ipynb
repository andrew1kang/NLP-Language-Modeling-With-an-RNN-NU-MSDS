{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSDS 422 - Andrew Kang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will utilize the chakin python package to download word embeddings.  The word embeddings are differentiated based on dimensions, source, and vocabulary size.  Once we download the word embeddings, we will proceed to create a loop for model training.  In the model loop, we will seek to create 4 models in a 2x2 experimental design that will test embedding dimension and vocabulary size combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import chakin  \n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "chakin.search(lang='English')  # lists available indices in English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Files From Chakin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAKIN_INDEX = 11\n",
    "NUMBER_OF_DIMENSIONS = 50\n",
    "SUBFOLDER_NAME = \"gloVe.6B\"\n",
    "\n",
    "DATA_FOLDER = \"embeddings\"\n",
    "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
    "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
    "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBFOLDER_NAME[-1] == \"d\":\n",
    "    GLOVE_FILENAME = os.path.join(\n",
    "        UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
    "else:\n",
    "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(\n",
    "        SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already downloaded.\n",
      "Embeddings already extracted.\n",
      "\n",
      "Run complete\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
    "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
    "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
    "else:\n",
    "    print(\"Embeddings already downloaded.\")\n",
    "\n",
    "if not os.path.exists(UNZIP_FOLDER):\n",
    "    import zipfile\n",
    "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
    "        ZIP_FILE = ZIP_FILE_ALT\n",
    "    with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n",
    "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
    "        zip_ref.extractall(UNZIP_FOLDER)\n",
    "else:\n",
    "    print(\"Embeddings already extracted.\")\n",
    "\n",
    "print('\\nRun complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os  # operating system functions\n",
    "import os.path  # for manipulation of file path names\n",
    "import re  # regular expressions\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import tensorflow as tf\n",
    "RANDOM_SEED = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "REMOVE_STOPWORDS = False  # no stopword removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        data = tf.compat.as_str(f.read())\n",
    "        data = data.lower()\n",
    "        data = text_parse(data)\n",
    "        data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "codelist = ['\\r', '\\n', '\\t']       \n",
    "# -----------------------------------------------\n",
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-negative'\n",
    "\n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "#     print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "    negative_documents.append(words)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "#     print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "#     print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "#     print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = ['Model 1 - 10000 w/ 50d', \n",
    "             'Model 2 - 30000 w/ 50d', \n",
    "             'Model 3 - 10000 w/ 100d', \n",
    "             'Model 4 - 30000 w/ 100d']\n",
    "trainAccuracy = []\n",
    "testAccuracy = []\n",
    "procTime = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loop for Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFullModel(dynamic_vocab_size,embeddings_filename):\n",
    "    print('\\nLoading embeddings from', embeddings_filename)\n",
    "    word_to_index, index_to_embedding = \\\n",
    "        load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "    print(\"Embedding loaded from disks.\")\n",
    "\n",
    "    EVOCABSIZE = dynamic_vocab_size \n",
    "    \n",
    "    def default_factory():\n",
    "        return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "\n",
    "    # dictionary has the items() function, returns list of (key, value) tuples\n",
    "    limited_word_to_index = defaultdict(default_factory, \\\n",
    "        {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "    \n",
    "    # Note: unknown words have representations with values [0, 0, ..., 0]\n",
    "    vocab_size, embedding_dim = index_to_embedding.shape\n",
    "\n",
    "    # Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "    limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "    # Set the unknown-word row to be all zeros as previously\n",
    "    limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "        index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "            reshape(1,embedding_dim), \n",
    "        axis = 0)\n",
    "\n",
    "    # create list of lists of lists for embeddings\n",
    "    embeddings = []    \n",
    "    for doc in documents:\n",
    "        embedding = []\n",
    "        for word in doc:\n",
    "            embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "        embeddings.append(embedding)\n",
    "  \n",
    "    # -----------------------------------------------------    \n",
    "    # Make embeddings a numpy array for use in an RNN \n",
    "    # Create training and test sets with Scikit Learn\n",
    "    # -----------------------------------------------------\n",
    "    embeddings_array = np.array(embeddings)\n",
    "\n",
    "    # Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "    thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                          np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "    # Scikit Learn for random splitting of the data  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Random splitting of the data in to training (80%) and test (20%)  \n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                         random_state = RANDOM_SEED)\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "    n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "    n_neurons = 10  # analyst specified number of neurons\n",
    "    n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "    y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "    logits = tf.layers.dense(states, n_outputs)\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    n_epochs = 50\n",
    "    batch_size = 50\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        start = datetime.now()\n",
    "        for epoch in range(n_epochs):\n",
    "#             print('---- Epoch ', epoch, ' ----')\n",
    "            for iteration in range(y_train.shape[0] // batch_size):          \n",
    "                X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "                y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "#                 print('  Batch ', iteration, ' training observations from ',  \n",
    "#                       iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "#             print('Epoch: ', epoch, 'Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
    "    end = datetime.now()\n",
    "    processing_time = end-start\n",
    "    trainAccuracy.append(acc_train)\n",
    "    testAccuracy.append(acc_test)\n",
    "    procTime.append(processing_time)\n",
    "    print(\"Train Accuracy: \",acc_train)\n",
    "    print(\"Test Accuracy: \",acc_test)\n",
    "    print(\"Processing Time: \", processing_time)\n",
    "    return acc_train,acc_test,processing_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe.6B.50d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - 10,000 Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.50d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n",
      "Train Accuracy:  0.8\n",
      "Test Accuracy:  0.71\n",
      "Processing Time:  0:00:02.933199\n"
     ]
    }
   ],
   "source": [
    "model_1 = runFullModel(10000,embeddings_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - 30,000 Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n",
      "Train Accuracy:  0.88\n",
      "Test Accuracy:  0.68\n",
      "Processing Time:  0:00:02.897786\n"
     ]
    }
   ],
   "source": [
    "model_2 = runFullModel(30000,embeddings_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe.6B.100d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - 10,000 Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.100d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.100d.txt\n",
      "Embedding loaded from disks.\n",
      "Train Accuracy:  0.78\n",
      "Test Accuracy:  0.605\n",
      "Processing Time:  0:00:03.955970\n"
     ]
    }
   ],
   "source": [
    "model_3 = runFullModel(10000,embeddings_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - 30,000 Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.100d.txt\n",
      "Embedding loaded from disks.\n",
      "Train Accuracy:  0.92\n",
      "Test Accuracy:  0.645\n",
      "Processing Time:  0:00:03.945530\n"
     ]
    }
   ],
   "source": [
    "model_4 = runFullModel(30000,embeddings_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"Models\":modelList})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance['Train Accuracy'] = trainAccuracy\n",
    "performance['Test Accuracy'] = testAccuracy\n",
    "performance['Processing Time'] = procTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = performance.sort_values(by='Test Accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Processing Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model 1 - 10000 w/ 50d</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.710</td>\n",
       "      <td>00:00:02.933199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model 2 - 30000 w/ 50d</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.680</td>\n",
       "      <td>00:00:02.897786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model 4 - 30000 w/ 100d</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.645</td>\n",
       "      <td>00:00:03.945530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model 3 - 10000 w/ 100d</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.605</td>\n",
       "      <td>00:00:03.955970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Models  Train Accuracy  Test Accuracy Processing Time\n",
       "0   Model 1 - 10000 w/ 50d            0.80          0.710 00:00:02.933199\n",
       "1   Model 2 - 30000 w/ 50d            0.88          0.680 00:00:02.897786\n",
       "3  Model 4 - 30000 w/ 100d            0.92          0.645 00:00:03.945530\n",
       "2  Model 3 - 10000 w/ 100d            0.78          0.605 00:00:03.955970"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our 2x2 experimental design, we tested two different embedding dimensions as well as two different vocab sizes.  What we found is that 50 embedding dimensions outperformed in our model relative to 100 embedding dimensions.  For vocab size, we actually had mixed performance.  For 50 embedding dimensions, we found that 10000 vocab size performed better.  For 100 embedding dimensions, we found that 30000 vocab size performed better.  Given our model parameters, 50 embedding dimensions and 10000 vocab size was the best performing model.  One thing that may improve our model performance for higher embedding dimensions may be the number of neurons.  For further analysis, we would want to test hyperparameters for neurons relative to embedding dimension size.\n",
    "\n",
    "The performance of our individual models in our experimental design is not ideal in terms of accuracy.  Our train accuracy and test accuracy deviated substantially, which suggests that there is overfitting occurring on the RNN.  Consequently, even though Model 1 was the victor in our model competition, our recommendation to business management would be to expand our experimental design to include other embedding dimensions, vocabulary sizes, neurons, and other hypereparameters before selecting a final model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
